# Settings >>inside<< container environments >>not<< docker compose file!

# These settings cannot be changed with command-line environment variables
# set on-the-fly before running `docker compose` for example DOES NOT WORK:
# `CELERY_THREAD_PER_WORKER=16 docker compose ...` - this is because the 
# stack deployment should be hard-configured in this file at the moment

# ==================
# Brick API Settings
# ==================

# Minimum disk space required
WORK_DISK_SPACE_GB=5

# Maximum size of a session directory
SESSION_MAX_SIZE_MB=200

# Maximum number of files in a session directory
SESSION_MAX_FILES=10000

# ==================
# Brick APP Settings
# ==================

# Sets the expected origin of requests
# for the Sveltekit server (form actions)

# https://kit.svelte.dev/docs/adapter-node#environment-variables

# Localhost and port for development deployment,
# domain for web deployment - note that DEV (5173) and
# PROD (5174) are on different ports so they can be run
# simultaneously as profiles in the docker-compose file

# Precedence for docker-compose environment block
# with localhost configuraton at the moment!

# ORIGIN: http://localhost:5174  

PUBLIC_BRICK_VERSION=0.3.0

# Request size limit to Sveltekit server
# determines maximum file and other request
# size (20MB)
BODY_SIZE_LIMIT=20000000

# This may need to be adjusted for longer
# running tasks or slow connections.

# Task results are checked with an exponential
# random backoff function, which adjusts the 
# initial PRIVATE_CELERY_TASK_CHECK_INTERVAL

# Timeout and interval defined in milliseconds

# Celery task result check timeout (10m)
PRIVATE_CELERY_TASK_CHECK_TIMEOUT=600000

# Celery task result check interval (500ms)
PRIVATE_CELERY_TASK_CHECK_INTERVAL=500


# =====================
# Brick WORKER Settings
# =====================

# Timeouts of task queue submitssions
# may need to be adjusted depending on
# long running task configurations, 
# soft timeout triggers first

# Timeout defined in seconds

CELERY_TASK_SOFT_TIMEOUT=12000
CELERY_TASK_HARD_TIMEOUT=18000 

# Mostly relevant to set resource limits on production server, 
# can generally be ignored on local machine.

# If not set, defaults to total number of cores on machine for threads
# available to each worker process (CELERY_THREADS_PER_WORKER) and:

# 1 thread per subprocess execution if CELERY_THREADS_PER_PROCESS <= 4
# and all threads per process if CELERY_THREADS_PER_PROCESS > 4, 

# Fallback of 1 thread for CELERY_THREADS_PER_WORKER and CELERY_THREADS_PER_PROCESS
# if number of cores on machine cannot be determined

# Note that hard resource limits may be enforced by the deploy directive for containers, 
# and limits on memory are generally set depending on largest database size. If container
# memory limits are exceeded, the worker will hard crash the container - a restart policy
# is recommended.

# CELERY_THREADS_PER_WORKER=8
# CELERY_THREADS_PER_PROCESS=8

# Memory control over `mmseqs` database in geNomad
# may significantly increase runtime, check if the
# timeout settings are appropriate:

# - PRIVATE_CELERY_TASK_CHECK_TIMEOUT
# - CELERY_TASK_SOFT_TIMEOUT
# - CELERY_TASK_HARD_TIMEOUT

# GENOMAD_SPLITS_ARG=4